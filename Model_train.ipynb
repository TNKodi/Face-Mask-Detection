{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428e4622",
   "metadata": {},
   "source": [
    "Build & Train CNN Model from Scratch\n",
    "\n",
    "**Goals:**\n",
    "1. Define a simple CNN architecture for 3‑class mask classification  \n",
    "2. Set up loss (`CrossEntropyLoss`), optimizer (`Adam`), and (optional) LR scheduler  \n",
    "3. Write train/validation loops  \n",
    "4. Track & plot loss/accuracy  \n",
    "5. Save the best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f4f40",
   "metadata": {},
   "source": [
    "## Import Libraries and Check Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7767e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU (ID: 0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\"\n",
    "          f\" (ID: {torch.cuda.current_device()})\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38a49d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=100352, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self,num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3,32, kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32,64, kernel_size=3,padding=1),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64,128, kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "\n",
    "        )\n",
    " \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*(224//8)*(224//8), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model= SimpleCNN(num_classes=3).to(device)\n",
    "print(model)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0bdac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52cd6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct   += (preds == labels).sum().item()\n",
    "        total     += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct   += (preds == labels).sum().item()\n",
    "            total     += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08f6793d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 75\n"
     ]
    }
   ],
   "source": [
    "# — Step 3 setup (must run before your training loop) —\n",
    "\n",
    "import os, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE    = 224\n",
    "BATCH_SIZE  = 8\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# (Re‑define your CLASS_MAP and collect_paths_and_labels if needed)\n",
    "CLASS_MAP = {\"with_mask\":0,\"mask_weared_incorrect\":1,\"without_mask\":2}\n",
    "def xml_to_label(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    return CLASS_MAP[tree.find(\"object/name\").text.lower()]\n",
    "\n",
    "def collect_paths_and_labels(split):\n",
    "    img_dir = f\"dataset/{split}/images\"\n",
    "    ann_dir = f\"dataset/{split}/annotations\"\n",
    "    paths, labels = [], []\n",
    "    for fn in os.listdir(img_dir):\n",
    "        if not fn.lower().endswith((\".png\",\".jpg\")): continue\n",
    "        xml = os.path.join(ann_dir, fn.rsplit(\".\",1)[0]+\".xml\")\n",
    "        if not os.path.exists(xml): continue\n",
    "        paths.append(os.path.join(img_dir,fn))\n",
    "        labels.append(xml_to_label(xml))\n",
    "    return paths, labels\n",
    "\n",
    "train_paths, train_labels = collect_paths_and_labels(\"train\")\n",
    "val_paths,   val_labels   = collect_paths_and_labels(\"val\")\n",
    "test_paths,  test_labels  = collect_paths_and_labels(\"test\")\n",
    "\n",
    "# Transforms\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(15),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform=None):\n",
    "        self.paths, self.labels, self.transform = paths, labels, transform\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.paths[i]).convert(\"RGB\")\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, self.labels[i]\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(MaskDataset(train_paths, train_labels, train_transform),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(MaskDataset(val_paths,   val_labels,   val_transform),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(MaskDataset(test_paths,  test_labels,  val_transform),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9ba902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/15  Train: loss=0.7786, acc=0.7973  Val:   loss=0.6500, acc=0.7874  Time: 12.0s\n",
      "Epoch 02/15  Train: loss=0.6257, acc=0.8141  Val:   loss=0.6568, acc=0.7874  Time: 13.3s\n",
      "Epoch 03/15  Train: loss=0.5582, acc=0.8124  Val:   loss=0.6397, acc=0.7874  Time: 13.7s\n",
      "Epoch 04/15  Train: loss=0.5923, acc=0.8141  Val:   loss=0.6310, acc=0.7874  Time: 12.0s\n",
      "Epoch 05/15  Train: loss=0.5584, acc=0.8141  Val:   loss=0.6359, acc=0.7874  Time: 11.8s\n",
      "Epoch 06/15  Train: loss=0.5852, acc=0.8124  Val:   loss=0.6744, acc=0.7874  Time: 11.9s\n",
      "Epoch 07/15  Train: loss=0.5536, acc=0.8157  Val:   loss=0.7206, acc=0.7874  Time: 12.2s\n",
      "Epoch 08/15  Train: loss=0.5566, acc=0.8124  Val:   loss=0.6710, acc=0.7953  Time: 12.2s\n",
      "Epoch 09/15  Train: loss=0.5601, acc=0.8141  Val:   loss=0.6214, acc=0.8031  Time: 12.6s\n",
      "Epoch 10/15  Train: loss=0.5133, acc=0.8208  Val:   loss=0.6408, acc=0.7953  Time: 12.2s\n",
      "Epoch 11/15  Train: loss=0.5002, acc=0.8208  Val:   loss=0.6305, acc=0.8031  Time: 12.2s\n",
      "Epoch 12/15  Train: loss=0.5036, acc=0.8342  Val:   loss=0.6870, acc=0.7874  Time: 12.5s\n",
      "Epoch 13/15  Train: loss=0.4776, acc=0.8409  Val:   loss=0.6747, acc=0.7795  Time: 12.3s\n",
      "Epoch 14/15  Train: loss=0.4281, acc=0.8459  Val:   loss=0.8556, acc=0.8031  Time: 12.4s\n",
      "Epoch 15/15  Train: loss=0.4523, acc=0.8559  Val:   loss=0.7748, acc=0.7953  Time: 13.0s\n",
      "\n",
      "Best validation accuracy: 0.8031\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss':   [], 'val_acc':   []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_path = \"best_mask_cnn.pth\"\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    start = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss,   val_acc   = validate(model, val_loader,   criterion, device)\n",
    "    # scheduler.step(val_loss)  # if using scheduler\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS}  \"\n",
    "          f\"Train: loss={train_loss:.4f}, acc={train_acc:.4f}  \"\n",
    "          f\"Val:   loss={val_loss:.4f}, acc={val_acc:.4f}  \"\n",
    "          f\"Time: {(time.time()-start):.1f}s\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a926b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# 1. Settings\n",
    "MODEL_PATH = \"best_mask_cnn.pth\"\n",
    "IMG_SIZE   = 224\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CLASS_NAMES = [\"Mask\", \"Worn Incorrectly\", \"No Mask\"]\n",
    "BOX_COLORS  = [(0,255,0), (0,165,255), (0,0,255)]  # green, orange, red\n",
    "\n",
    "# 2. Load your trained model\n",
    "class SimpleMaskCNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(32, 64, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(64,128,3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "        )\n",
    "        flattened = 128 * (IMG_SIZE//8) * (IMG_SIZE//8)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(flattened, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = SimpleMaskCNN(num_classes=3).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# 3. Define the same transforms you used in training\n",
    "transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# 4. Load OpenCV’s Haar‐cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# 5. Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "fps_display_interval = 5  # seconds\n",
    "frame_count = 0\n",
    "start_time = time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        # Crop & preprocess face\n",
    "        face_img = frame[y:y+h, x:x+w]\n",
    "        pil_img = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "        inp = transform(pil_img).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            logits = model(inp)\n",
    "            pred = logits.argmax(dim=1).item()\n",
    "        \n",
    "        # Draw box & label\n",
    "        color = BOX_COLORS[pred]\n",
    "        label = CLASS_NAMES[pred]\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), color, 2)\n",
    "        cv2.putText(frame, label, (x, y-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "    \n",
    "    # Compute & display FPS\n",
    "    frame_count += 1\n",
    "    elapsed = time() - start_time\n",
    "    if elapsed > fps_display_interval:\n",
    "        fps = frame_count / elapsed\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10,30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,0), 2)\n",
    "        start_time = time()\n",
    "        frame_count = 0\n",
    "    \n",
    "    # Show\n",
    "    cv2.imshow(\"Real‑Time Face Mask Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10846844",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     90\u001b[39m     cv2.imshow(\u001b[33m\"\u001b[39m\u001b[33mTest Set Playback\u001b[39m\u001b[33m\"\u001b[39m, frame)\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# wait 500ms between frames; press ESC to quit early\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m)\u001b[49m & \u001b[32m0xFF\u001b[39m == \u001b[32m27\u001b[39m:\n\u001b[32m     93\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     95\u001b[39m cv2.destroyAllWindows()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# ─── Settings ──────────────────────────────────────────────────────────────\n",
    "IMG_SIZE   = 224\n",
    "BATCH_SIZE = 1\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"best_mask_cnn.pth\"\n",
    "\n",
    "CLASS_NAMES = [\"Mask\", \"Incorrectly Worn\", \"No Mask\"]\n",
    "BOX_COLORS  = [(0,255,0), (0,165,255), (0,0,255)]  # green, orange, red\n",
    "\n",
    "TEST_IMG_DIR  = \"dataset/test/images\"\n",
    "TEST_ANN_DIR  = \"dataset/test/annotations\"\n",
    "\n",
    "# ─── Load Model ─────────────────────────────────────────────────────────────\n",
    "class SimpleMaskCNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(32, 64, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(64,128,3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "        )\n",
    "        flat_dim = 128 * (IMG_SIZE//8) * (IMG_SIZE//8)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(flat_dim, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = SimpleMaskCNN(num_classes=3).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# ─── Transforms ─────────────────────────────────────────────────────────────\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# ─── XML Parser ─────────────────────────────────────────────────────────────\n",
    "def parse_annotation(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    boxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        bb = obj.find('bndbox')\n",
    "        xmin = int(bb.find('xmin').text)\n",
    "        ymin = int(bb.find('ymin').text)\n",
    "        xmax = int(bb.find('xmax').text)\n",
    "        ymax = int(bb.find('ymax').text)\n",
    "        boxes.append((xmin, ymin, xmax, ymax))\n",
    "    return boxes\n",
    "\n",
    "# ─── Play Test Images as “Video” ───────────────────────────────────────────\n",
    "for fname in sorted(os.listdir(TEST_IMG_DIR)):\n",
    "    if not fname.lower().endswith((\".png\",\".jpg\")): continue\n",
    "    img_path = os.path.join(TEST_IMG_DIR, fname)\n",
    "    ann_path = os.path.join(TEST_ANN_DIR, fname.rsplit(\".\",1)[0] + \".xml\")\n",
    "    if not os.path.exists(ann_path): continue\n",
    "\n",
    "    frame = cv2.imread(img_path)\n",
    "    for (xmin,ymin,xmax,ymax) in parse_annotation(ann_path):\n",
    "        face = frame[ymin:ymax, xmin:xmax]\n",
    "        pil   = Image.fromarray(cv2.cvtColor(face,cv2.COLOR_BGR2RGB))\n",
    "        inp   = transform(pil).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            pred = model(inp).argmax(dim=1).item()\n",
    "\n",
    "        color = BOX_COLORS[pred]\n",
    "        label = CLASS_NAMES[pred]\n",
    "        cv2.rectangle(frame, (xmin,ymin), (xmax,ymax), color, 2)\n",
    "        cv2.putText(frame, label, (xmin, ymin-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Test Set Playback\", frame)\n",
    "    # wait 500ms between frames; press ESC to quit early\n",
    "    if cv2.waitKey(500) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
